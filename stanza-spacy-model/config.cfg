[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
seed = 0
gpu_allocator = null

[nlp]
lang = "en"
pipeline = ["negex"]
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
batch_size = 1000

[nlp.tokenizer]
@tokenizers = "spacy_stanza.PipelineAsTokenizer.v1"
lang = "en"
dir = null
package = "mimic"
logging_level = null
verbose = null
use_gpu = true

[nlp.tokenizer.kwargs]

[nlp.tokenizer.processors]
ner = "i2b2"

[components]

[components.negex]
factory = "negex"
chunk_prefix = ["no"]
ent_types = []
extension_name = "negex"

[components.negex.neg_termset]
pseudo_negations = ["no further","not able to be","not certain if","not certain whether","not necessarily","without any further","without difficulty","without further","might not","not only","no increase","no significant change","no change","no definite change","not extend","not cause","gram negative","not rule out","not ruled out","not been ruled out","not drain","no suspicious change","no interval change","no significant interval change"]
preceding_negations = ["no cause of","aren't","werent","resolved","declined","ruled out","doubt","denies","no evidence of","rule him out","isn't","negative","rule out","not taking","arent","ruled her out","ruled the patient out","didn't","never","evaluate for","rule the patient out","never developed","patient was not","free of","without any reactions or signs of","without","without sign of","no complaints of","not demonstrate","weren't","cannot","rules out","denied","no signs of","dont","has not had","no","couldnt","start","not","isnt","ruled him out","versus","without signs of","didnt","can't","negatives","begin","denying","without indication of","does not take","no sign of","couldn't","fails to reveal","never had","ruled patient out","rule her out","cant","did not exhibit","ro","r/o","absence of","consider","symptoms atypical","has no history","negative for","wasn't","doesn't","don't","doesnt","rule patient out","wasnt"]
following_negations = ["was ruled out","werent","declined","not tried","denies","negative","was not","unlikely","not taking","none","weren't","denied","no","denying","deny","absent","were not","wasn't","free","wasnt","were ruled out"]
termination = ["although","apart from","as there are","aside from","but","except","however","involving","nevertheless","still","though","which","yet","cause for","cause of","causes for","causes of","etiology for","etiology of","origin for","origin of","origins for","origins of","other possibilities of","reason for","reason of","reasons for","reasons of","secondary to","source for","source of","sources for","sources of","trigger event for"]

[corpora]

[corpora.dev]
@readers = "spacy.Corpus.v1"
path = ${paths.dev}
gold_preproc = false
max_length = 0
limit = 0
augmenter = null

[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}
gold_preproc = false
max_length = 0
limit = 0
augmenter = null

[training]
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 1600
max_epochs = 0
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
dev_corpus = "corpora.dev"
train_corpus = "corpora.train"
before_to_disk = null

[training.batcher]
@batchers = "spacy.batch_by_words.v1"
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = "compounding.v1"
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.logger]
@loggers = "spacy.ConsoleLogger.v1"
progress_bar = false

[training.optimizer]
@optimizers = "Adam.v1"
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]